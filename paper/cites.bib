@phdthesis{MacCartney2009,
author = {MacCartney, Bill},
file = {:home/mr-zhou/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/MacCartney - 2009 - Natural Language Inference.pdf:pdf},
mendeley-groups = {Recognizing Textual Entailment},
pages = {179},
title = {{Natural Language Inference}},
url = {https://nlp.stanford.edu/{~}wcmac/papers/nli-diss.pdf},
year = {2009}
}

// 深度学习在nlp中的应用
@inproceedings{Kneser1995Improved,
  title={Improved Backing-off for N-gram Language Modeling},
  author={Kneser, Reinhard and Ney, Hermann},
  booktitle={International Conference on Acoustics, Speech, and Signal Processing},
  pages={181-184 vol.1},
  year={1995},
}
@INPROCEEDINGS{nnlm:2001:nips,
    author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal},
     title = {A Neural Probabilistic Language Model},
      year = {2001},
       url = {http://www.iro.umontreal.ca/~lisa/pointeurs/nips00_lm.ps},
  crossref = {NIPS13},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words. This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons. In the proposed approach one learns simultaneously (1) a distributed representation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these representations. Generalization is obtained because a sequence of words that
has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model.},
topics={Markov,Unsupervised,Language},cat={C},
}
@proceedings{DBLP:conf/interspeech/2010,
  editor    = {Takao Kobayashi and
               Keikichi Hirose and
               Satoshi Nakamura},
  title     = {{INTERSPEECH} 2010, 11th Annual Conference of the International Speech
               Communication Association, Makuhari, Chiba, Japan, September 26-30,
               2010},
  publisher = {{ISCA}},
  year      = {2010},
  timestamp = {Thu, 24 Aug 2017 11:26:53 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/interspeech/2010},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/Graves13,
  author    = {Alex Graves},
  title     = {Generating Sequences With Recurrent Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1308.0850},
  year      = {2013},
  url       = {http://arxiv.org/abs/1308.0850},
  archivePrefix = {arXiv},
  eprint    = {1308.0850},
  timestamp = {Mon, 13 Aug 2018 16:47:21 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Graves13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/MelisDB17,
  author    = {G{\'{a}}bor Melis and
               Chris Dyer and
               Phil Blunsom},
  title     = {On the State of the Art of Evaluation in Neural Language Models},
  journal   = {CoRR},
  volume    = {abs/1707.05589},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.05589},
  archivePrefix = {arXiv},
  eprint    = {1707.05589},
  timestamp = {Mon, 13 Aug 2018 16:47:01 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MelisDB17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Mikolov2013_1,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {1301.3781},
file = {:home/mr-zhou/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
mendeley-groups = {Natural Language Process,Natural Language Process/Models},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/abs/1301.3781},
year = {2013}
}
@article{Mikolov2013_2,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:home/mr-zhou/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
isbn = {2150-8097},
issn = {10495258},
mendeley-groups = {Natural Language Process,Natural Language Process/Models},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {http://arxiv.org/abs/1310.4546},
year = {2013}
}
@article{Pennington2014,
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
file = {:home/mr-zhou/文档/glove.pdf:pdf},
mendeley-groups = {Natural Language Process/Models},
title = {{GloVe: Global Vectors for Word Representation}},
year = {2014}
}
@inproceedings{DBLP:conf/nips/LevyG14,
  author    = {Omer Levy and
               Yoav Goldberg},
  title     = {Neural Word Embedding as Implicit Matrix Factorization},
  booktitle = {Advances in Neural Information Processing Systems 27: Annual Conference
               on Neural Information Processing Systems 2014, December 8-13 2014,
               Montreal, Quebec, Canada},
  pages     = {2177--2185},
  year      = {2014},
  crossref  = {DBLP:conf/nips/2014},
  url       = {http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization},
  timestamp = {Wed, 10 Dec 2014 21:34:12 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/nips/LevyG14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/tacl/LevyGD15,
  author    = {Omer Levy and
               Yoav Goldberg and
               Ido Dagan},
  title     = {Improving Distributional Similarity with Lessons Learned from Word
               Embeddings},
  journal   = {{TACL}},
  volume    = {3},
  pages     = {211--225},
  year      = {2015},
  url       = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570},
  timestamp = {Thu, 28 May 2015 10:59:41 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/tacl/LevyGD15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/KirosZSZTUF15,
  author    = {Ryan Kiros and
               Yukun Zhu and
               Ruslan Salakhutdinov and
               Richard S. Zemel and
               Antonio Torralba and
               Raquel Urtasun and
               Sanja Fidler},
  title     = {Skip-Thought Vectors},
  journal   = {CoRR},
  volume    = {abs/1506.06726},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.06726},
  archivePrefix = {arXiv},
  eprint    = {1506.06726},
  timestamp = {Mon, 13 Aug 2018 16:48:27 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KirosZSZTUF15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/BahdanauCB14,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  journal   = {CoRR},
  volume    = {abs/1409.0473},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.0473},
  archivePrefix = {arXiv},
  eprint    = {1409.0473},
  timestamp = {Mon, 13 Aug 2018 16:46:05 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BahdanauCB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/SutskeverVL14,
  author    = {Ilya Sutskever and
               Oriol Vinyals and
               Quoc V. Le},
  title     = {Sequence to Sequence Learning with Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1409.3215},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.3215},
  archivePrefix = {arXiv},
  eprint    = {1409.3215},
  timestamp = {Mon, 13 Aug 2018 16:48:06 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SutskeverVL14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/WuSCLNMKCGMKSJL16,
  author    = {Yonghui Wu and
               Mike Schuster and
               Zhifeng Chen and
               Quoc V. Le and
               Mohammad Norouzi and
               Wolfgang Macherey and
               Maxim Krikun and
               Yuan Cao and
               Qin Gao and
               Klaus Macherey and
               Jeff Klingner and
               Apurva Shah and
               Melvin Johnson and
               Xiaobing Liu and
               Lukasz Kaiser and
               Stephan Gouws and
               Yoshikiyo Kato and
               Taku Kudo and
               Hideto Kazawa and
               Keith Stevens and
               George Kurian and
               Nishant Patil and
               Wei Wang and
               Cliff Young and
               Jason Smith and
               Jason Riesa and
               Alex Rudnick and
               Oriol Vinyals and
               Greg Corrado and
               Macduff Hughes and
               Jeffrey Dean},
  title     = {Google's Neural Machine Translation System: Bridging the Gap between
               Human and Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1609.08144},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.08144},
  archivePrefix = {arXiv},
  eprint    = {1609.08144},
  timestamp = {Mon, 13 Aug 2018 16:46:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/WuSCLNMKCGMKSJL16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Kalchbrenner2016Neural,
  title={Neural Machine Translation in Linear Time},
  author={Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and Oord, Aaron Van Den and Graves, Alex and Kavukcuoglu, Koray},
  year={2016},
}
@article{Gehring2017Convolutional,
  title={Convolutional Sequence to Sequence Learning},
  author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N.},
  year={2017},
}

@article{Vaswani2017Attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year={2017},
}

@article{Chen2018The,
  title={The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation},
  author={Chen, Mia Xu and Firat, Orhan and Bapna, Ankur and Johnson, Melvin and Macherey, Wolfgang and Foster, George and Jones, Llion and Parmar, Niki and Schuster, Mike and Chen, Zhifeng},
  year={2018},
}

@InProceedings{N18-1101,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for 
           Sentence Understanding through Inference",
  booktitle = "Proceedings of the 2018 Conference of 
               the North American Chapter of the 
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
  url = "http://aclweb.org/anthology/N18-1101"
}

@article{Chen2016,
abstract = {Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6{\%} on the Stanford Natural Language Inference Dataset. Unlike the previous top models that use very complicated network architectures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best result---it further improves the performance even when added to the already very strong model.},
archivePrefix = {arXiv},
arxivId = {1609.06038},
author = {Chen, Qian and Zhu, Xiaodan and Ling, Zhenhua and Wei, Si and Jiang, Hui and Inkpen, Diana},
doi = {10.1016/0923-2494(90)90036-X},
eprint = {1609.06038},
file = {:home/mr-zhou/文档/P17-1152.pdf:pdf},
issn = {09232494},
journal = {Research in Immunology},
mendeley-groups = {Recognizing Textual Entailment},
month = {sep},
number = {4},
pages = {431--440},
title = {{Enhanced LSTM for Natural Language Inference}},
url = {http://arxiv.org/abs/1609.06038},
volume = {141},
year = {2016}
}

@article{Peters2018,
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
archivePrefix = {arXiv},
arxivId = {1802.05365},
author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
eprint = {1802.05365},
file = {:home/mr-zhou/文档/1802.05365.pdf:pdf},
mendeley-groups = {Natural Language Process,Natural Language Process/Models},
month = {feb},
title = {{Deep contextualized word representations}},
url = {http://arxiv.org/abs/1802.05365},
year = {2018}
}

@article{Gong2017,
abstract = {Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus. It's noteworthy that DIIN achieve a greater than 20{\%} error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.},
archivePrefix = {arXiv},
arxivId = {1709.04348},
author = {Gong, Yichen and Luo, Heng and Zhang, Jian},
eprint = {1709.04348},
file = {:home/mr-zhou/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gong, Luo, Zhang - 2017 - Natural Language Inference over Interaction Space.pdf:pdf},
mendeley-groups = {Recognizing Textual Entailment},
month = {sep},
number = {2017},
pages = {1--15},
title = {{Natural Language Inference over Interaction Space}},
url = {http://arxiv.org/abs/1709.04348},
year = {2017}
}

@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
doi = {10.1007/s10107-014-0839-0},
eprint = {1409.3215},
file = {:home/mr-zhou/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutskever, Vinyals, Le - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:pdf},
isbn = {1409.3215},
issn = {09205691},
mendeley-groups = {Natural Language Process,Natural Language Process/Models},
pages = {1--9},
pmid = {2079951},
title = {{Sequence to Sequence Learning with Neural Networks}},
url = {http://arxiv.org/abs/1409.3215},
year = {2014}
}

@article{DBLP:journals/corr/LuongPM15,
  author    = {Minh{-}Thang Luong and
               Hieu Pham and
               Christopher D. Manning},
  title     = {Effective Approaches to Attention-based Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1508.04025},
  year      = {2015},
  url       = {http://arxiv.org/abs/1508.04025},
  archivePrefix = {arXiv},
  eprint    = {1508.04025},
  timestamp = {Mon, 13 Aug 2018 16:46:14 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LuongPM15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{2017,
author = {郭茂盛 and 张宇 and 刘挺},
file = {:home/mr-zhou/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/郭茂盛, 张宇, 刘挺 - 2017 - 文本蕴含关系识别与知识获取研究进展及展望.pdf:pdf},
journal = {计算机学报},
mendeley-groups = {Recognizing Textual Entailment},
title = {文本蕴含关系识别与知识获取研究进展及展望},
year = {2017}
}

@article{Bowman2015,
abstract = {Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.},
archivePrefix = {arXiv},
arxivId = {1508.05326},
author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
doi = {10.18653/v1/D16-1264},
eprint = {1508.05326},
file = {:home/mr-zhou/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bowman et al. - 2015 - A large annotated corpus for learning natural language inference.pdf:pdf},
isbn = {9781941643327},
issn = {9781941643327},
mendeley-groups = {Recognizing Textual Entailment},
pmid = {299497},
title = {{A large annotated corpus for learning natural language inference}},
url = {http://arxiv.org/abs/1508.05326},
year = {2015}
}

@article{DBLP:journals/corr/VendrovKFU15,
  author    = {Ivan Vendrov and
               Ryan Kiros and
               Sanja Fidler and
               Raquel Urtasun},
  title     = {Order-Embeddings of Images and Language},
  journal   = {CoRR},
  volume    = {abs/1511.06361},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06361},
  archivePrefix = {arXiv},
  eprint    = {1511.06361},
  timestamp = {Mon, 13 Aug 2018 16:49:01 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/VendrovKFU15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mou2015recognizing,
  title={Recognizing entailment and contradiction by tree-based convolution},
  author={Mou, Lili and Men, Rui and Li, Ge and Xu, Yan and Zhang, Lu and Yan, Rui and Jin, Zhi},
  journal={CoRR, abs/1512.08422},
  volume={2},
  year={2015}
}

@article{bowman2016fast,
  title={A fast unified model for parsing and sentence understanding},
  author={Bowman, Samuel R and Gauthier, Jon and Rastogi, Abhinav and Gupta, Raghav and Manning, Christopher D and Potts, Christopher},
  journal={arXiv preprint arXiv:1603.06021},
  year={2016}
}

@article{rocktaschel2015reasoning,
  title={Reasoning about entailment with neural attention},
  author={Rockt{\"a}schel, Tim and Grefenstette, Edward and Hermann, Karl Moritz and Ko{\v{c}}isk{\`y}, Tom{\'a}{\v{s}} and Blunsom, Phil},
  journal={arXiv preprint arXiv:1509.06664},
  year={2015}
}

@article{wang2015learning,
  title={Learning natural language inference with LSTM},
  author={Wang, Shuohang and Jiang, Jing},
  journal={arXiv preprint arXiv:1512.08849},
  year={2015}
}

@article{cheng2016long,
  title={Long short-term memory-networks for machine reading},
  author={Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
  journal={arXiv preprint arXiv:1601.06733},
  year={2016}
}

@book{quinonero2006machine,
  title={Machine Learning Challenges: Evaluating Predictive Uncertainty, Visual Object Classification, and Recognizing Textual Entailment, First Pascal Machine Learning Challenges Workshop, MLCW 2005, Southampton, UK, April 11-13, 2005, Revised Selected Papers},
  author={Qui{\~n}onero-Candela, Joaquin and Dagan, Ido and Magnini, Bernardo and D'Alch{\'e}-Buc, Florence},
  volume={3944},
  year={2006},
  publisher={Springer}
}

@incollection{zhang2014chinese,
  title={Chinese textual entailment recognition based on syntactic tree clipping},
  author={Zhang, Zhichang and Yao, Dongren and Chen, Songyi and Ma, Huifang},
  booktitle={Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data},
  pages={83--94},
  year={2014},
  publisher={Springer}
}

@article{2015基于混合主题模型的文本蕴涵识别,
  title={基于混合主题模型的文本蕴涵识别},
  journal={计算机工程},
  volume={41},
  number={5},
  pages={180-184},
  year={2015},
}

@article{赵红燕2014多特征文本蕴涵识别研究,
  title={多特征文本蕴涵识别研究},
  author={赵红燕 and 刘鹏 and 李茹 and 王智强},
  journal={中文信息学报},
  volume={28},
  number={2},
  pages={109-115},
  year={2014},
}

@article{huang2017exploring,
  title={Exploring lexical, syntactic, and semantic features for Chinese textual entailment in NTCIR RITE evaluation tasks},
  author={Huang, Wei-Jie and Liu, Chao-Lin},
  journal={Soft Computing},
  volume={21},
  number={2},
  pages={311--330},
  year={2017},
  publisher={Springer}
}

@inproceedings{jijkoun2005recognizing,
  title={Recognizing textual entailment using lexical similarity},
  author={Jijkoun, Valentin and de Rijke, Maarten and others},
  booktitle={Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment},
  pages={73--76},
  year={2005},
  organization={Citeseer}
}

@inproceedings{adams2007textual,
  title={Textual entailment through extended lexical overlap and lexico-semantic matching},
  author={Adams, Rod and Nicolae, Gabriel and Nicolae, Cristina and Harabagiu, Sanda},
  booktitle={Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing},
  pages={119--124},
  year={2007},
  organization={Association for Computational Linguistics}
}

@article{mehdad2009edits,
  title={Edits: An open source framework for recognizing textual entailment},
  author={Mehdad, Mehdad and Matteo, Negri and Elena, Cabrio and Milen, Kouylekov and Magnini, Bernardo},
  year={2009}
}

@article{任函2015基于知识话题模型的文本蕴涵识别,
  title={基于知识话题模型的文本蕴涵识别},
  author={任函 and 盛雅琦 and 冯文贺 and 刘茂福},
  journal={中文信息学报},
  volume={29},
  number={6},
  pages={119-126},
  year={2015},
}

@inproceedings{maccartney2008phrase,
  title={A phrase-based alignment model for natural language inference},
  author={MacCartney, Bill and Galley, Michel and Manning, Christopher D},
  booktitle={Proceedings of the conference on empirical methods in natural language processing},
  pages={802--811},
  year={2008},
  organization={Association for Computational Linguistics}
}

@article{basak2015recognizing,
  title={Recognizing textual entailment by soft dependency tree matching},
  author={Basak, Rohini and Naskar, Sudip Kumar and Pakray, Partha and Gelbukh, Alexander},
  journal={Computaci{\'o}n y Sistemas},
  volume={19},
  number={4},
  pages={685--700},
  year={2015},
  publisher={Centro de Investigaci{\'o}n en Computaci{\'o}n, IPN}
}

@inproceedings{sultan2015feature,
  title={Feature-rich two-stage logistic regression for monolingual alignment},
  author={Sultan, Md Arafat and Bethard, Steven and Sumner, Tamara},
  booktitle={Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  pages={949--959},
  year={2015}
}

@inproceedings{noh2015multi,
  title={Multi-level alignments as an extensible representation basis for textual entailment algorithms},
  author={Noh, Tae-Gil and Pad{\'o}, Sebastian and Shwartz, Vered and Dagan, Ido and Nastase, Vivi and Eichler, Kathrin and Kotlerman, Lili and Adler, Meni},
  booktitle={Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics},
  pages={193--198},
  year={2015}
}

@article{marneffe2008finding,
  title={Finding contradictions in text},
  author={Marneffe, Marie-Catherine and Rafferty, Anna N and Manning, Christopher D},
  journal={Proceedings of ACL-08: HLT},
  pages={1039--1047},
  year={2008}
}

@article{hobbs1993interpretation,
  title={Interpretation as abduction},
  author={Hobbs, Jerry R and Stickel, Mark E and Appelt, Douglas E and Martin, Paul},
  journal={Artificial intelligence},
  volume={63},
  number={1-2},
  pages={69--142},
  year={1993},
  publisher={Elsevier}
}

@inproceedings{raina2005robust,
  title={Robust textual inference via learning and abductive reasoning},
  author={Raina, Rajat and Ng, Andrew Y and Manning, Christopher D},
  booktitle={AAAI},
  pages={1099--1105},
  year={2005}
}

@inproceedings{moldovan2003cogex,
  title={Cogex: A logic prover for question answering},
  author={Moldovan, Dan and Clark, Christine and Harabagiu, Sanda and Maiorano, Steve},
  booktitle={Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1},
  pages={87--93},
  year={2003},
  organization={Association for Computational Linguistics}
}

@inproceedings{akhmatova2005textual,
  title={Textual entailment resolution via atomic propositions},
  author={Akhmatova, Elena},
  booktitle={Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment},
  volume={150},
  year={2005},
  organization={Citeseer}
}

@phdthesis{roy2017reasoning,
  title={Reasoning about quantities in natural language},
  author={Roy, Subhro},
  year={2017},
  school={University of Illinois at Urbana-Champaign}
}

@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called $\backslash$Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error through $\backslash$constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
eprint = {1206.2944},
file = {:C$\backslash$:/Users/zhoujianbo/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hochreiter, Urgen Schmidhuber - 1997 - Long Short-Term Memory.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural Computation},
mendeley-groups = {Natural Language Process,Natural Language Process/Models},
month = {nov},
number = {8},
pages = {1735--1780},
pmid = {9377276},
title = {{Long Short-Term Memory}},
url = {http://www7.informatik.tu-muenchen.de/{~}hochreit{\%}5Cnhttp://www.idsia.ch/{~}juergen http://arxiv.org/abs/1206.2944 http://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
volume = {9},
year = {1997}
}

@incollection{graves2012supervised,
  title={Supervised sequence labelling},
  author={Graves, Alex},
  booktitle={Supervised sequence labelling with recurrent neural networks},
  pages={5--13},
  year={2012},
  publisher={Springer}
}

@book{hinton1984distributed,
  title={Distributed representations},
  author={Hinton, Geoffrey E and McClelland, James L and Rumelhart, David E and others},
  year={1984},
  publisher={Carnegie-Mellon University Pittsburgh, PA}
}