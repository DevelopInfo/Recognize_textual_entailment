---
bibliography: [cites.bib]
---

# 1. 选题依据与价值

## 1.1 课题背景

随着自然语言处理(Natural Language Processing, NLP)领域研究的不断深入，学者们的研究已经不只停留在词法解析、句法解析等表层信息的处理上了，让机器理解语义正渐渐成为学者们的研究目标，而自然语言推理作为自然语言理解的重要组成部分，也成为自然语言处理的一项研究热点。

自然语言推理(Natural Language Inference, NLI)通常是指识别文本间的蕴含关系，所以通常也称为文本蕴含关系识别(Recognize Textual Entailment, RTE)。

## 1.2 应用价值

文本间的蕴含关系普遍存在于自然语言文本之间，所以文本蕴含关系识别在许多自然语言处理领域都存在着实际应用价值。本节将列出一些文本蕴含关系识别的应用[@MacCartney2009]。

**问答系统** 一个理想的文本蕴含系统可以作为问答系统的答案验证模块，用来判断一个从知识库种抽取的候选答案是否能够推断出目标问题或是目标问题的陈述句形式。例如，某人提问“港珠澳大桥的总工程师是谁？”，通过信息检索技术从知识库中检索出候选答案“林鸣，自2010年12月起，担任港珠澳大桥岛隧工程项目总经理、总工程师”，利用文本蕴含系统可以验证是否可以从候选答案推断出目标问题。为检验文本蕴含系统的这种能力，欧洲跨语言评测平台(Cross Language Evaluation Forum, CLEF)在2006-2008举办了三届答案验证评测(Answer Validation Exercise, AVE)。在评测中，每一条数据由一个问题、一个候选答案和一篇支持文本组成，文本蕴含模型的目标就是在给定支持文本的情景下，针对目标问题，判断候选答案是否正确。

**语义检索** 语义检索的目标是基于检索语句的语义从大量的文档库中检索出目标文档的目标语句。如果将文本蕴含系统运用在语义检索中，它可以用来识别目标问题与目标源文档中各个语句的语义相似性。例如，用户搜索“人们示威反对自由贸易”，现存的基于关键字的搜索引擎将会返回包含“示威”、“自由贸易”等关键字的文档，如果文档中仅存在像“游行人员高举‘反对解除贸易壁垒协议’的标语”这样不含关键字的语句，那将不会被检索到，而造成信息遗漏，利用文本蕴含识别可以避免上述问题。

**自动摘要** 在自动摘要任务中，一个主要的挑战是怎样消除冗余。冗余现象在多文档摘要中特别明显，多文档摘要是融合了多篇源文档的内容，而多篇文档之间的内容可能存在语义相似的，从而造成自动生成的摘要中存在多条语句表达相似语义的问题。文本蕴含识别系统用来判断是否存在语句所表达的语义可以由其他摘要语句推理得到。自动摘要的另一个挑战就是正确性，也就是摘要应该准确地反映单个源文档或多个源文档的内容。文本蕴含识别系统可以通过判断源文档是否可以推理出摘要，以确保摘要的正确性。

**机器翻译的自动评估** 文本蕴含识别的一个相对比较新的应用就是自动评估机器翻译的性能。目前，机器翻译研究者主要利用BLEU作为翻译好坏的指标，BLEU是通过衡量机器翻译语句与人类翻译语句之间的重合程度来，并没有考虑语义层次的相似性。而文本蕴含识别可以判断机器翻译的语句是否可以推断出人类翻译的语句，若可以推断出，则可以认为这是一个好的翻译，即使机器翻译的句子与人类翻译的句子之间没有一个字是一样的。

# 2. 国内外研究现状

## 2.1 文本蕴含识别研究现状



# 3. 相关理论和技术
## 循环神经网络(Recurrent Neural Network, RNN)

### 长短记忆神经网络(Long Short-term Memory, LSTM)

### 双向长短记忆神经网络(Bidirectional LSTM, BiLSTM)

## Attention机制


## ELMo词嵌入模型[@Peters2018]
&emsp;&emsp;ELMo全称Embeddings from Language Models，是一种目前最先进的词嵌入模型，在多个自然语言理解的数据集上都有较好的表现，在性能指标上都有显著的提升。预训练文本表示在自然语言理解领域具有重要地位，其中Word2vec[@Mikolov2013_1][@Mikolov2013_2]和Glove[@Pennington2014]对预训练词表示起到了关键作用。但是Word2vec和Glove都只是将一个词用一个向量表示，而词在不同的语境中会有不同的语义，而用一个固定的向量来表示一个词则无法体现词的多义性。ELMo的则是引入了一种基于语境的词嵌入方式。<br>
&emsp;&emsp;ELMo模型是基于大量文本语料，从深层的双向语言模型（deep bidirectional language model）中的内部状态(internal state)训练而来的。深层双向语言模型由多层双向长短记忆网络(BiLSTM)构建而成。ELMo是将BiLSTM中的各层状态向量进行线性组合而得。对于某个词$t_{k}$，可以由一个$L$层的BiLSTM中的$L+1$个状态向量表示如下：
$$R_{k} = \{h^{LM}_{k,i} | i=0, ..., L\}$$
&emsp;&emsp;其中,$h^{LM}_{k,i}$表示BiLSTM第i层的状态向量。然后ELMo将$R_{k}$组合成一个词向量$ELMo^{task}_{k}$，实验发现[@Peters2018]，将$R_{k}$中的所有向量进行加权求和组合并进行缩放具有最好的效果，具体的组合公式如下：
$$ELMo^{task}_{k} = E(R_{k},\Theta^{task}) = \gamma^{task}\sum^{L}_{i=0}s^{task}_{i}h^{LM}_{k,i}$$
&emsp;&emsp;其中$s^{task}=softmax(w^{task})$是归一化后的权重向量,$\gamma^{task}\in \mathbb{R}$可以对整个词向量进行放缩调整。这两个参数都是根据具体任务参与训练的，而$h^{LM}_{k}$从预训练双向语言模型中抽取获得的。<br>
&emsp;&emsp;当在问答系统、文本蕴含、机器翻译等监督学习任务中，ELMo词向量$ELMo^{task}_{k}$可以与普通的预训练词向量$x_{k}$进行合并得$[x_{k};ELMo^{task}_{k}]$来表示一个词；同时，实验表明将具体任务（例如：SNLI,SQuAD）中的RNN模型的输出$h_{k}$替换成$[h_{k};ELMo^{task}_{k}]$，会获得更好的实验效果。

## Transformer模型[@Vaswani2017Attention]
&emsp;&emsp;Transformer是一种新的Seq2Seq网络结构，完全依赖Attention机制，舍弃了循环神经网和卷积操作，在机器翻译领域取得重大成功。Transformer整体网络结构如图所示：
<center>
<img src="pics/Transformer.png" alt="图：Transformer整体结构(Vaswani 等人于2017年发表)" width="400"/></br>
图：Transformer整体结构(Ashish 等人于2017年发表)</br>
</center>
</br>
&emsp;&emsp;Transformer主要由编码器(Encoder)与解码器(Decoder)组成，由于本文主要采用Transformer的编码器部分，所以下文将主要阐述Transformer的编码器结构。Transformer的编码器主要由多个编码层堆叠而成，每个编码层结构相同，上层编码层的输出作为下层的输入。每个编码层的分别由两部分组成：第一部分是多头自注意力层(Multi-head Self-attention)，第二部分是全连接神经网络(Position-wise feed-forward network)。两个部分都有一个残差链接(residual connection)，然后接着一个 Layer Normalization 层。

### Multi-head Self-attention 机制
&emsp;&emsp;本节先介绍Scaled Dot-product Attention 机制。Vaswani 等人[@Vaswani2017Attention]在论文中曾将Attention机制描述为一个问题(query)与一组键值对(key-value pairs)的函数，函数的输出则是由键值(value)的加权求和得到，其中权重则是由问题(query)与对应的关键字(key)计算得到。设问题(query)$Q \in \mathbb{R^{n \times d}}$，键值对的关键字(key)$K \in \mathbb{R^{m \times d}}$，键值对的键值(value)$V \in \mathbb{R^{m \times d}}$，Scaled Dot-product Attention函数的输出$Attention(Q,K,V)$，则函数输出的计算公式如下：
$$Attention(Q,K,V) = softmax(\frac{QK^{T}}{\sqrt{d}})V$$
&emsp;&emsp;Scaled Dot-product Attention与普通的基于內积的Attention机制的唯一区别就在于scaled dot-product attention 有一个缩放因子$\frac{1}{\sqrt{d}}$，$d$表示词向量的维度，当词向量维度很大时，$QK^{T}$就会偏大，使得softmax函数处于梯度较小的区域，而乘上缩放因子，可以在一定程度上修正这种情况。<br>
&emsp;&emsp;为在多个层次上获取语义关联，Vaswani 等人[@Vaswani2017Attention]提出Multi-head Attention结构，该机制将问题$Q$,键值对中的关键字$K$和键值$V$分别进行多次线性变换之后，进行Scaled Dot-product Attention操作，再将结果进行合并操作，再次经过线性变换，得到最后最终的输出。Multi-head Attention的网络结构如下图所示：
<center>
<img src="pics/Multi-Head_Attention.png" alt="图：Multi-head Attention网络结构" width="400"/></br>
图：Multi-head Attention网络结构</br>
</center>
</br>
&emsp;&emsp;Multi-head Attention 使得模型能够获取来自不同位置的多个表示子空间的信息。Multi-head Attention操作的公式如下：

$$MultiHead(Q,K,V) = Concat(head_{1}, ..., head_{h})W^{o}$$
$$head_{i} = Attention(QW^{Q}_{i}, KW^{k}_{i}, VW^{V}_{i})$$

&emsp;&emsp;其中$W^{o} \in \mathbb{R^{d \times d}},W^{Q}_{i} \in \mathbb{R^{d \times d/h}}, W^{k}_{i} \in \mathbb{R^{d \times d/h}}, W^{V}_{i} \in \mathbb{R^{d \times d/h}}$分别表示线性变换的权重，$h \in \mathbb{Z^{+}}$表示将Q、K、V 进行线性映射的次数。原始模型使用$h=8$个并行的attention 层。<br>
&emsp;&emsp;当Multi-head Attention中的问题Q，键值对中的关键字K与键值V均对应着同一个序列时，K=V=Q，则称这种结构为Multi-head Self-attention。这样做的主要是为了学习序列内部各个元素的依赖关系，捕获序列中的内部结构。

### Position-wise Feed-forward network
&emsp;&emsp;Transformer的编码器部分的子层不仅有一个Multi-head Self-attention 层还包括一个前馈神经网络(Feed-forward network)。这个前馈神经网络分别对序列的每个位置的元素做了两次线性变换，两次线性变换之间运用ReLU激活函数。公式如下：
$$FFN(x) = ReLU(xW_{1}+b_{1})W_{2}+b_{2}$$
&emsp;&emsp;其中，全连接层的权重与偏置针对序列中每个位置的元素是共享的。上述操作也可以理解为卷积核尺寸为1的两层一维卷积操作。

### 位置编码
&emsp;&emsp;由于Transformer模型中不包含循环神经网络和卷积操作，所以丢失了序列的时序信息，因此，Transformer模型的输入需要加入时序信息。原模型中使用三角函数对时序信息进行编码，公式如下：
$$PE(pos,i) = \left\{\begin{matrix}
sin(pos/10000^{i/d}) & i=2k,k \in \mathbb{N} & \\ 
cos(pos/10000^{i-1/d})&  i=2k + 1 & 
\end{matrix}\right.$$
$$$$
&emsp;&emsp;其中$pos$表示元素在序列中的位置，$i$表示元素向量的维度。对一个序列中的每个元素都使用sin函数与cos函数相结合的方式进行编码，针对用向量表示的每个元素，对向量的奇数维用cos函数进行编码，对向量的偶数维用sin函数编码。

# 研究内容与实施方案
## 研究内容
### 课题描述

### 数据集介绍
#### SNLI数据集
#### MultiNLI数据集
&emsp;&emsp;MultiNLI (Multi-Genre Natural Language Inference) 数据集[@N18-1101]是目前最大的文本蕴含数据集，是Stanford所推出的自然语言推理数据集SNLI的升级版本。MultiNLI数据集总共有433000组句子样本，包含10种不同的领域，风格上包括文本和口语两种英文风格。

## 实施方案
&emsp;&emsp;鉴于Chen 等人所建立的增强序列推理模型(Enhanced Sequential Inference Model, ESIM)ESIM[@Chen2016]在文本蕴含数据集SNLI与MutiNLI上的成功，本文将在ESIM模型基础上中提出基于Bi-LSTM与Attention的文本蕴含识别模型，并希望能够对模型的并行计算能力与数据性能指标进行改善。下文将对模型的主要部分进行描述并提出改进计划。

### 基本模型结构
&emsp;&emsp;文本蕴含任务可以抽象为一个三分类任务，模型大致可分为6层，分别为：序列输入层、序列编码层、序列间相关信息抽取层、信息融合层、特征提取层和分类层。序列输入层对文本序列进行向量化的表示；序列编码层将融合词的上下文信息；序列间相关信息抽取层将从假设序列中提取与前提序列相关的信息，同理也从前提序列中提取与假设序列相关的信息；信息融合层将两个序列的信息进行整合；特征提取层提取语义特征信息；分类层将提取后的特征信息映射到类别标签。模型的结构如图所示：


#### 序列输入层
&emsp;&emsp;对于序列输入层，本文仅使用预训练的词向量表，并对词向量表中未出现的词进行随机初始化。通过词向量映射表将字符映射为向量，我们得到前提序列$p = \{p_{1}, p_{2}, ... , p_{l_{p}}\}$ 与假设序列 $h = \{h_{1}, h_{2}, ... , h_{l_{h}}\}$，其中$p_{i}, h_{j} \in \mathbb{R^{e}}$。

#### 序列编码层
&emsp;&emsp;为了让序列中每个元素都融合上下文信息，模型利用双向长短记忆网络(Bi-LSTM)分别对前提序列和假设序列进行编码，编码过程可由如下公式所示：
$$\bar{p}_{i} = BiLSTM(p,i), \forall i \in [1, 2, ..., l_{p}]$$ 
$$\bar{h}_{j} = BiLSTM(h,j), \forall j \in [1, 2, ..., l_{h}]$$ 
&emsp;&emsp;其中 $\bar{p}_{i}, \bar{h}_{j} \in \mathbb{R^{h}}$ 分别是一个h维的向量，分别是前提序列的第i个词向量和假设序列的第j个词向量在经过双向长短记忆网络后输出的隐状态向量。双向长短记忆神经网络每个时刻输出的隐状态向量不仅具有序列在该时刻词的信息，并且还包含该时刻词的上下文信息。

#### 序列间相关信息抽取层
&emsp;&emsp;对输入序列进行编码之后，前提序列与假设序列分别由 $\bar{p} = \{\bar{p}_{1}, ...,\bar{p}_{l_{p}}\}$ 和 $\bar{h} = \{\bar{h}_{1}, ...,\bar{h}_{l_{h}}\}$表示，每个被编码的序列元素包含着对应时刻词的信息和其上下文的信息，但两个序列之间的联系并没有描述，模型需要对两个序列的关系进行建模描述。局部推理模块利用软注意力(soft attention)机制对两个被编码的序列的各个元素之间的关系进行建模。为了判断假设序列中的每个词是否可以从前提序列中推理出来，软注意力机制首先计算两个序列各个元素之间的相似度$e \in \mathbb{R^{l_{p} \times l_{h}}}$，然后通过softmax操作得到一个权重向量$w_{p \to h} \in \mathbb{R^{l_{h} \times l_{p}}}$，再对前提序列中的每个元素进行加权求和，得到前提序列针对假设序列上下文向量$\tilde{h} \in \mathbb{R^{l_{h}}}$。同理可得，假设序列针对前提序列的上下文向量$\tilde{p} \in \mathbb{R^{l_{p}}}$。上述过程可由如下公式表示：
$$\tilde{p}_{i} = \sum^{l_{h}}_{j=1}\frac{exp(e_{ij})}{\sum^{l_{p}}_{k=1}exp(e_{ik})}\bar{h}_{j}, \forall i \in [1, ..., l_{p}]$$
$$\tilde{h}_{j} = \sum^{l_{p}}_{i=1}\frac{exp(e_{ij})}{\sum^{l_{h}}_{k=1}exp(e_{kj})}\bar{p}_{i}, \forall j \in [1, ..., l_{h}]$$
&emsp;&emsp;其中$e_{ij}$表示前提序列的第i个元素与假设序列的第j个元素相关程度，计算元素之间关联程度的方式有很多，本文通过最简单的方式，计算两序列中各个词向量之间的内积来衡量两词之间关联程度，公式如下：
$$e_{ij} = \bar{p}_{i}^{T}\bar{b}_{j}$$
&emsp;&emsp;实验表明[@Chen2016]，其他一些更加复杂的衡量词之间关联程度的方式并没有改善实验性能。对于两个序列的各个元素之间的相似度计算可利用张量(Tensor)运算进行并行计算。

#### 信息

&emsp;&emsp;然后模型对序列的各方面的信息进行融合。针对前提序列，本模型对编码后的矩阵$\bar{p} \in \mathbb{R^{l_{p} \times h}}$ 和假设序列的上下文矩阵$\tilde{p} \in \mathbb{R^{l_{p} \times h}}$ 做了一些操作后，之后进行合并，过程如下公式所示：
$$m_{p} = [\bar{p};\tilde{p};\bar{p}-\tilde{p};\bar{p} \odot \tilde{p}]$$
&emsp;&emsp;其中$\odot$表示矩阵中对应位置的元素相乘。同理可得，假设序列的信息融合过程如下公式所示：
$$m_{h} = [\bar{h};\tilde{h};\bar{h}-\tilde{h};\bar{h} \odot \tilde{h}]$$
#### 组合推理模块(Inference Composition)
&emsp;&emsp;局部推理信息形成之后，模型需要对局部推理信息进行进一步的组合，使得每个时刻的元素都包含上下文的信息。本模型
$$v_{p,i} = BiLSTM(m_{p},i), \forall i \in [1, 2, ..., l_{p}]$$ 
$$v_{h,j} = BiLSTM(m_{h},j), \forall j \in [1, 2, ..., l_{h}]$$ 
&emsp;&emsp;最后的推理模块利用池化与合并操作将处理后的两条序列信息转化为一个固定长度的向量，并且通过分类器对两序列的蕴含关系进行分类。池化操作主要是将平均池化和最大池化相结合，然后将池化后的向量进行合并操作，形成一个长度固定的向量$v \in \mathbb{R^{4h}}$。向量$v$的计算过程如下：
$$v_{p,ave} =  \frac{1}{l_{p}}\sum_{i=1}^{l_{p}}v_{p,i}$$
$$v_{p,max} = max_{i=1}^{l_{p}}v_{p,i}$$ 
$$v_{h,ave} =  \frac{1}{l_{h}}\sum_{j=1}^{l_{h}}v_{h,i}$$ 
$$v_{h,max} = max_{i=1}^{l_{h}}v_{h,j}$$ 
$$v = [v_{p,ave};v_{p,max};v_{h,ave};v_{h,max}] \in \mathbb{R^{4h}}$$
&emsp;&emsp;最后，模型分类器利用多层感知机(Multi-Layer Perceptron)对句子蕴含关系进行分类。本模型的多层感知机只有一个隐藏层，激活函数是tanh，最后输出层利用softmax进行数据归一化。分类的计算过程如下：
$$out = softmax(tanh(Wv+b))$$
&emsp;&emsp;其中$W \in \mathbb{R^{3 \times 4h}}$和$b \in \mathbb{R^{3}}$分别是全连接层的权重和偏置，输出向量$out \in \mathbb{R^{3}}, out_i \in (0,1), \forall i \in [1,2,3]$中的每一个元素表示相对应蕴含关系的概率。

### 实验细节与部分实验结果
&emsp;&emsp;本文我们选用 300-D Glove 840B 中预训练的词向量来初始词向量。对于Glove中未出现的词，我们对其词向量进行随机初始化。训练过程中，我们选用Adam来优化梯度下降算法，其中Adam优化器中的超参数$\beta_{1}=0.9, \beta_{2}=0.999$，初始学习率$\eta=0.0004$，每次训练数据的数目$batch\_size=32$。模型参数方面，所有的长短记忆神经网络(LSTM)的隐状态向量和初始词向量的维度都是300维。我们在每层全连接层的输入处应用dropout机制，以加快训练速度并防止过拟合，dropout的比例均为0.5。<br>
&emsp;&emsp;目前，我们分别在数据集SNLI和MultiNLI数据集上进行了训练并测试。训练及测试结果如表所示：
<center>
表：实验结果</br>
<table border="1">
<tr>
<td>Train Corpus</td>
<td>Model</td>
<td>Embedding-Dimention</td>
<td>Params</td>
<td>Train Acc</td>
<td>SNLI Dev Acc</td>
<td>MNLI Dev-matched Acc</td>
<td>MNLI Dev-mismatched Acc</td>
</tr>
</table>
</center>
</br>

### 改进计划
#### 文本表示
&emsp;&emsp;增强序列推理模型(ESIM)利用词向量对文本进行表示，而一个向量表示一个词，不足以体现词的多义性。而ELMo模型使用多个词向量的线性组合表示一个词，不同层次的词向量表示语义的不同层次。当词在不同语境下时，多个层次的词向量通过不同的线性组合对词进行表示，这样使得词具备语境信息。 

#### 序列编码
&emsp;&emsp;

#### 序列元素影响可视化

# 参考文献



