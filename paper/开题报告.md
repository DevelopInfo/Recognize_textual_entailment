---
bibliography: [cites.bib]
---

# 1. 选题依据与价值

## 1.1 课题背景

随着自然语言处理(Natural Language Processing, NLP)领域研究的不断深入，学者们的研究已经不只停留在词法解析、句法解析等表层信息的处理上了，让机器理解语义正渐渐成为学者们的研究目标，而自然语言推理作为自然语言理解的重要组成部分，也成为自然语言处理的一项研究热点。

自然语言推理(Natural Language Inference, NLI)通常是指识别文本间的蕴含关系，所以通常也称为文本蕴含关系识别(Recognize Textual Entailment, RTE)。

## 1.2 应用价值

文本间的蕴含关系普遍存在于自然语言文本之间，所以文本蕴含关系识别在许多自然语言处理领域都存在着实际应用价值。本节将列出一些文本蕴含关系识别的应用[@MacCartney2009]。

**问答系统** 一个理想的文本蕴含系统可以作为问答系统的答案验证模块，用来判断一个从知识库种抽取的候选答案是否能够推断出目标问题或是目标问题的陈述句形式。例如，某人提问“港珠澳大桥的总工程师是谁？”，通过信息检索技术从知识库中检索出候选答案“林鸣，自2010年12月起，担任港珠澳大桥岛隧工程项目总经理、总工程师”，利用文本蕴含系统可以验证是否可以从候选答案推断出目标问题。为检验文本蕴含系统的这种能力，欧洲跨语言评测平台(Cross Language Evaluation Forum, CLEF)在2006-2008举办了三届答案验证评测(Answer Validation Exercise, AVE)。在评测中，每一条数据由一个问题、一个候选答案和一篇支持文本组成，文本蕴含模型的目标就是在给定支持文本的情景下，针对目标问题，判断候选答案是否正确。

**语义检索** 语义检索的目标是基于检索语句的语义从大量的文档库中检索出目标文档的目标语句。如果将文本蕴含系统运用在语义检索中，它可以用来识别目标问题与目标源文档中各个语句的语义相似性。例如，用户搜索“人们示威反对自由贸易”，现存的基于关键字的搜索引擎将会返回包含“示威”、“自由贸易”等关键字的文档，如果文档中仅存在像“游行人员高举‘反对解除贸易壁垒协议’的标语”这样不含关键字的语句，那将不会被检索到，而造成信息遗漏，利用文本蕴含识别可以避免上述问题。

**自动摘要** 在自动摘要任务中，一个主要的挑战是怎样消除冗余。冗余现象在多文档摘要中特别明显，多文档摘要是融合了多篇源文档的内容，而多篇文档之间的内容可能存在语义相似的，从而造成自动生成的摘要中存在多条语句表达相似语义的问题。文本蕴含识别系统用来判断是否存在语句所表达的语义可以由其他摘要语句推理得到。自动摘要的另一个挑战就是正确性，也就是摘要应该准确地反映单个源文档或多个源文档的内容。文本蕴含识别系统可以通过判断源文档是否可以推理出摘要，以确保摘要的正确性。

**机器翻译的自动评估** 文本蕴含识别的一个相对比较新的应用就是自动评估机器翻译的性能。目前，机器翻译研究者主要利用BLEU作为翻译好坏的指标，BLEU是通过衡量机器翻译语句与人类翻译语句之间的重合程度来，并没有考虑语义层次的相似性。而文本蕴含识别可以判断机器翻译的语句是否可以推断出人类翻译的语句，若可以推断出，则可以认为这是一个好的翻译，即使机器翻译的句子与人类翻译的句子之间没有一个字是一样的。

# 2. 国内外研究现状

## 2.1 文本蕴含识别研究现状

## 2.2 深度学习在自然语言理解领域的研究现状

**神经语言模型(Neural Language Model)** 深度学习在自然语言处理领域的第一次影响深远的运用是利用神经网路构建语言模型。语言模型的任务是在已知一句话的前几个词情况下，来预测下一个词。语言模型在自然语言处理领域具有非常重要的地位，在一些简单的自然语言处理实践中被广泛应用，例如：智能拼写、拼写纠错，邮件回复建议。传统的语言模型主要是基于n元语法，并利用数据平滑算法对未登录词进行处理[@Kneser1995Improved]。2001年 Bengio 等人[@nnlm:2001:nips]第一次提出利用前馈神经网络构建语言模型，模型结构如图1所示：
<center>
<img src="pics/lm_bengio_2003.png" alt="图1：神经语言模型(Bengio 等人于2001年发表)" width="400"/></br>
图1：神经语言模型(Bengio 等人于2001年发表)</br>
</center>
</br>该模型将一段文本的前n个词在词表中的索引作为输入，然后在词表C中查找索引所对应的词向量，这种技术现在输出为一个与词表长度一样长的向量，向量的每一位代表对应词是第n+1个词的概率。之后，Mikolov 等人[@DBLP:conf/interspeech/2010]尝试利用循环神经网代替全连接神经网为语言模型建模，Graves 等人在此基础上又将循环神经网替换为长短记忆神经网（Long short-term memory neural network，LSTM）[@DBLP:journals/corr/Graves13]。直到现在，虽然神经语言模型一直在发展，但是基于LSTM的建模方式依然具有很强的竞争力[@DBLP:journals/corr/MelisDB17]。</br>

神经语言模型虽然功能很简单，但是对后续的很多自然语言处理领域的发展具有深远意义。在词向量研究方面，词向量的训练过程就是一个语言模型的简化。在序列到序列模型中，模型的输出是每个时刻预测一个词，最后形成一串序列，这样的形式也是受到语言模型的启发。在之后的预训练语言模型中，这些预训练的模型的预训练过程都是在训练一个语言模型，以求能够从中理解语义，然后进行迁移学习。

**多任务学习** 

**词嵌入(Word Embedding)** 

&emsp;&emsp;自然语言处理领域在过去很长时间里都是应用词袋模型，利用稀疏向量对文本进行表示。2001年，学者们开始利用稠密向量表示词。2013年，Mikolov 等人[@Mikolov2013_1][@Mikolov2013_2]提出两种利用神经网络训练词向量的模型，分别是CBOW和Skip-gram，这两种模型可以更加有效地训练词向量。CBOW模型的基本思想是通过中心词的上下文预测中心词，而Skip-gram模型是通过中心词来预测上下文。两个模型的结构如图2所示：
<center>
<img src="pics/cbow_skipgram_mikolov_2013.png" alt="图2：CBOW模型和Skip-gram模型的结构(Mikolov 等人于2013年发表)" width="400"/></br>
图1：神经语言模型(Bengio 等人于2001年发表)</br>
</center>
</br>
&emsp;&emsp;集成这两种模型的开源工具word2vec在很多自然语言处理应用中取得很好的效果。利用word2vec工具，Mikolov 等人[@Mikolov2013_1][@Mikolov2013_2]在大数据集上训练后，发现词向量之间存在着一定的语义关系，例如：词向量"king"减"man"加"woman"可以得到词向量"queue"。</br>
&emsp;&emsp;2014年，Pennington 等人[@Pennington2014]提出Glove模型，利用矩阵分解的方法训练词向量。相比于word2vec中集成的两个模型，Glove模型引入了全局信息，通过全局矩阵分解和局部的上下文窗口两种方式来提升词向量对语义的表达能力。同时，通过矩阵分解训练的词向量[@Pennington2014][@DBLP:conf/nips/LevyG14][@DBLP:journals/tacl/LevyGD15]也可以获得类似word2vec训练出的词向量所具有的语义关系。</br>

**神经网络在自然语言处理中的应用**

循环神经网络(Recurrent Neural Network)、卷积神经网络(Convolution Neural Netword)及递归神经网络(Recursive Neural Network)

**序列到序列(Sequence to Sequence) 模型框架**

&emsp;&emsp;在自然语言处理领域，有很多序列到序列的任务，例如：机器翻译、语音识别、摘要自动生成等任务。2014年， Sutskever 等人[@DBLP:journals/corr/SutskeverVL14]首次提出序列到序列的模型，这是一个将一个序列映射到另一个序列的通用模型框架，模型结构如图所示：
<center>
<img src="pics/sutskever_nips2014.png" alt="图：序列到序列模型(Sutskever 等人于2014年发表)" width="400"/></br>
图：序列到序列模型(Sutskever 等人于2014年发表)</br>
</center>
</br>
&emsp;&emsp;序列到序列模型由一个编码器和解码器构成，编码器将输入序列压缩成一个定长的稠密向量，解码器将编码器输出的稠密向量解码成任务所需要的输出序列。编码器和解码器的网络结构主要是基于循环神经网络，在此基础上也发展出一些变体。目前，序列到序列模型的编码器网络结构有长短记忆神经网络(LSTM)[@DBLP:journals/corr/WuSCLNMKCGMKSJL16]、卷积神经网络(CNN)[@Kalchbrenner2016Neural][@Gehring2017Convolutional]、Transfomer[@Vaswani2017Attention]以及一些混合结构[@Chen2018The]，解码器部分的网络结构与编码器类似，不过鲜有论文采用卷积神经网络结构。

**注意力机制(Attention)**

&emsp;&emsp;注意力机制(Attention Mechanism)是一个能够有效改善模型效果的机制，其在计算机视觉及自然语言处理领域均有广泛应用。2015年，Bahdanau 等人[@DBLP:journals/corr/BahdanauCB14]首次应用在机器翻译中Attention机制，这个关键创新使得基于神经网络的机器翻译在性能上远远超过传统模型。

**记忆网络(Memory-based Networks)**

**预训练模型**

# 3. 相关理论和技术
## 循环神经网络(Recurrent Neural Network, RNN)

### 长短记忆神经网络(Long Short-term Memory, LSTM)

### 双向长短记忆神经网络(Bidirectional LSTM, BiLSTM)

## Attention机制


## ELMo词嵌入模型[@Peters2018]
&emsp;&emsp;ELMo全称Embeddings from Language Models，是一种目前最先进的词嵌入模型，在多个自然语言理解的数据集上都有较好的表现，在性能指标上都有显著的提升。预训练文本表示在自然语言理解领域具有重要地位，其中Word2vec[@Mikolov2013_1][@Mikolov2013_2]和Glove[@Pennington2014]对预训练词表示起到了关键作用。但是Word2vec和Glove都只是将一个词用一个向量表示，而词在不同的语境中会有不同的语义，而用一个固定的向量来表示一个词则无法体现词的多义性。ELMo的则是引入了一种基于语境的词嵌入方式。<br>
&emsp;&emsp;ELMo模型是基于大量文本语料，从深层的双向语言模型（deep bidirectional language model）中的内部状态(internal state)训练而来的。深层双向语言模型由多层双向长短记忆网络(BiLSTM)构建而成。ELMo是将BiLSTM中的各层状态向量进行线性组合而得。对于某个词$t_{k}$，可以由一个$L$层的BiLSTM中的$L+1$个状态向量表示如下：
$$R_{k} = \{h^{LM}_{k,i} | i=0, ..., L\}$$
&emsp;&emsp;其中,$h^{LM}_{k,i}$表示BiLSTM第i层的状态向量。然后ELMo将$R_{k}$组合成一个词向量$ELMo^{task}_{k}$，实验发现[@Peters2018]，将$R_{k}$中的所有向量进行加权求和组合并进行缩放具有最好的效果，具体的组合公式如下：
$$ELMo^{task}_{k} = E(R_{k},\Theta^{task}) = \gamma^{task}\sum^{L}_{i=0}s^{task}_{i}h^{LM}_{k,i}$$
&emsp;&emsp;其中$s^{task}=softmax(w^{task})$是归一化后的权重向量,$\gamma^{task}\in \mathbb{R}$可以对整个词向量进行放缩调整。这两个参数都是根据具体任务参与训练的，而$h^{LM}_{k}$从预训练双向语言模型中抽取获得的。<br>
&emsp;&emsp;当在问答系统、文本蕴含、机器翻译等监督学习任务中，ELMo词向量$ELMo^{task}_{k}$可以与普通的预训练词向量$x_{k}$进行合并得$[x_{k};ELMo^{task}_{k}]$来表示一个词；同时，实验表明将具体任务（例如：SNLI,SQuAD）中的RNN模型的输出$h_{k}$替换成$[h_{k};ELMo^{task}_{k}]$，会获得更好的实验效果。

## Transformer模型[@Vaswani2017Attention]
&emsp;&emsp;Transformer是一种新的Seq2Seq网络结构，完全依赖Attention机制，舍弃了循环神经网和卷积操作，在机器翻译领域取得重大成功。Transformer整体网络结构如图所示：
<center>
<img src="pics/Transformer.png" alt="图：Transformer整体结构(Vaswani 等人于2017年发表)" width="400"/></br>
图：Transformer整体结构(Ashish 等人于2017年发表)</br>
</center>
</br>
&emsp;&emsp;Transformer主要由编码器(Encoder)与解码器(Decoder)组成，由于本文主要采用Transformer的编码器部分，所以下文将主要阐述Transformer的编码器结构。Transformer的编码器主要由多个编码层堆叠而成，每个编码层结构相同，上层编码层的输出作为下层的输入。每个编码层的分别由两部分组成：第一部分是多头自注意力层(Multi-head Self-attention)，第二部分是全连接神经网络(Position-wise feed-forward network)。两个部分都有一个残差链接(residual connection)，然后接着一个 Layer Normalization 层。

### Multi-head Self-attention 机制
&emsp;&emsp;本节先介绍Scaled Dot-product Attention 机制。Vaswani 等人[@Vaswani2017Attention]在论文中曾将Attention机制描述为一个问题(query)与一组键值对(key-value pairs)的函数，函数的输出则是由键值(value)的加权求和得到，其中权重则是由问题(query)与对应的关键字(key)计算得到。设问题(query)$Q \in \mathbb{R^{n \times d}}$，键值对的关键字(key)$K \in \mathbb{R^{m \times d}}$，键值对的键值(value)$V \in \mathbb{R^{m \times d}}$，Scaled Dot-product Attention函数的输出$Attention(Q,K,V)$，则函数输出的计算公式如下：
$$Attention(Q,K,V) = softmax(\frac{QK^{T}}{\sqrt{d}})V$$
&emsp;&emsp;Scaled Dot-product Attention与普通的基于內积的Attention机制的唯一区别就在于scaled dot-product attention 有一个缩放因子$\frac{1}{\sqrt{d}}$，$d$表示词向量的维度，当词向量维度很大时，$QK^{T}$就会偏大，使得softmax函数处于梯度较小的区域，而乘上缩放因子，可以在一定程度上修正这种情况。<br>
&emsp;&emsp;为在多个层次上获取语义关联，Vaswani 等人[@Vaswani2017Attention]提出Multi-head Attention结构，该机制将问题$Q$,键值对中的关键字$K$和键值$V$分别进行多次线性变换之后，进行Scaled Dot-product Attention操作，再将结果进行合并操作，再次经过线性变换，得到最后最终的输出。Multi-head Attention的网络结构如下图所示：
<center>
<img src="pics/Multi-Head_Attention.png" alt="图：Multi-head Attention网络结构" width="400"/></br>
图：Multi-head Attention网络结构</br>
</center>
</br>
&emsp;&emsp;Multi-head Attention 使得模型能够获取来自不同位置的多个表示子空间的信息。Multi-head Attention操作的公式如下：

$$MultiHead(Q,K,V) = Concat(head_{1}, ..., head_{h})W^{o}$$
$$head_{i} = Attention(QW^{Q}_{i}, KW^{k}_{i}, VW^{V}_{i})$$

&emsp;&emsp;其中$W^{o} \in \mathbb{R^{d \times d}},W^{Q}_{i} \in \mathbb{R^{d \times d/h}}, W^{k}_{i} \in \mathbb{R^{d \times d/h}}, W^{V}_{i} \in \mathbb{R^{d \times d/h}}$分别表示线性变换的权重，$h \in \mathbb{Z^{+}}$表示将Q、K、V 进行线性映射的次数。原始模型使用$h=8$个并行的attention 层。<br>
&emsp;&emsp;当Multi-head Attention中的问题Q，键值对中的关键字K与键值V均对应着同一个序列时，K=V=Q，则称这种结构为Multi-head Self-attention。这样做的主要是为了学习序列内部各个元素的依赖关系，捕获序列中的内部结构。

### Position-wise Feed-forward network
&emsp;&emsp;Transformer的编码器部分的子层不仅有一个Multi-head Self-attention 层还包括一个前馈神经网络(Feed-forward network)。这个前馈神经网络分别对序列的每个位置的元素做了两次线性变换，两次线性变换之间运用ReLU激活函数。公式如下：
$$FFN(x) = ReLU(xW_{1}+b_{1})W_{2}+b_{2}$$
&emsp;&emsp;其中，全连接层的权重与偏置针对序列中每个位置的元素是共享的。上述操作也可以理解为卷积核尺寸为1的两层一维卷积操作。

### 位置编码
&emsp;&emsp;由于Transformer模型中不包含循环神经网络和卷积操作，所以丢失了序列的时序信息，因此，Transformer模型的输入需要加入时序信息。原模型中使用三角函数对时序信息进行编码，公式如下：
$$PE(pos,i) = \left\{\begin{matrix}
sin(pos/10000^{i/d}) & i=2k,k \in \mathbb{N} & \\ 
cos(pos/10000^{i-1/d})&  i=2k + 1 & 
\end{matrix}\right.$$
$$$$
&emsp;&emsp;其中$pos$表示元素在序列中的位置，$i$表示元素向量的维度。对一个序列中的每个元素都使用sin函数与cos函数相结合的方式进行编码，针对用向量表示的每个元素，对向量的奇数维用cos函数进行编码，对向量的偶数维用sin函数编码。

# 研究内容与实施方案
## 研究内容
### 课题描述

### 数据集介绍
#### SNLI数据集
#### MultiNLI数据集
&emsp;&emsp;MultiNLI (Multi-Genre Natural Language Inference) 数据集[@N18-1101]是目前最大的文本蕴含数据集，是Stanford所推出的自然语言推理数据集SNLI的升级版本。MultiNLI数据集总共有433000组句子样本，包含10种不同的领域，风格上包括文本和口语两种英文风格。

## 实施方案
&emsp;&emsp;鉴于Chen 等人所建立的增强序列推理模型(Enhanced Sequential Inference Model, ESIM)ESIM[@Chen2016]在文本蕴含数据集SNLI与MutiNLI上的成功，本文将选择ESIM模型作为基准模型，并希望能够对模型的并行计算能力与数据性能指标进行改善。下文将对ESIM模型的主要部分进行描述并提出改进计划。

### ESIM模型
&emsp;&emsp;ESIM模型曾在snli数据集上取得最高的性能指标，并被选为MultiNLI数据集的官方基准模型，具有很高的参考价值。ESIM模型主要由三个部分组成：输入编码模块(Input Encoding), 局部推理模块(Local Inference Modeling)和组合推理模块(Inference Composition)。

#### 输入编码模块(Input Encoding)
&emsp;&emsp;文本蕴含问题是判断前提(premise)与假设(hypothesis)这两句话之间是否存在语义的蕴含关系。所以，模型首先需要对两个文本序列进行处理。本文规定 $p = \{p_{1}, p_{2}, ... , p_{l_{p}}\}$ 与 $h = \{h_{1}, h_{2}, ... , h_{l_{h}}\}$ 分别表示前提序列和假设序列，其中 $p_{i}, h_{j} \in \mathbb{R^{n}}$ 分别是一个维度为n的向量，分别表示前提序列的第i个词与假设序列的第j个词，这些词向量一般通过预训练的词向量进行初始化。然后模型利用双向长短记忆网络分别对前提序列和假设序列进行编码，编码过程可由如下公式表示：
$$\bar{p}_{i} = BiLSTM(p,i), \forall i \in [1, 2, ..., l_{p}]$$ 
$$\bar{h}_{j} = BiLSTM(h,j), \forall j \in [1, 2, ..., l_{h}]$$ 
&emsp;&emsp;其中 $\bar{p}_{i}, \bar{h}_{j} \in \mathbb{R^{h}}$ 分别是一个h维的向量，分别是前提序列的第i个词向量和假设序列的第j个词向量在经过双向长短记忆网络后输出的隐状态向量。双向长短记忆神经网络每个时刻输出的隐状态向量不仅具有序列在该时刻词的信息，并且还包含该时刻词的上下文信息。

#### 局部推理模块(Local Inference Modeling)
&emsp;&emsp;对输入序列进行编码之后，前提序列与假设序列分别由 $\bar{p} = \{\bar{p}_{1}, ...,\bar{p}_{l_{p}}\}$ 和 $\bar{h} = \{\bar{h}_{1}, ...,\bar{h}_{l_{h}}\}$表示，每个被编码的序列元素包含着对应时刻词的信息和其上下文的信息，但两个序列之间的联系并没有描述，模型需要对两个序列的关系进行建模描述。局部推理模块利用软注意力(soft attention)机制对两个被编码的序列的各个元素之间的关系进行建模。为了判断假设序列中的每个词是否可以从前提序列中推理出来，软注意力机制首先计算两个序列各个元素之间的相似度$e \in \mathbb{R^{l_{p} \times l_{h}}}$，然后通过softmax操作得到一个权重向量$w_{p \to h} \in \mathbb{R^{l_{h} \times l_{p}}}$，再对前提序列中的每个元素进行加权求和，得到前提序列针对假设序列上下文向量$\tilde{h} \in \mathbb{R^{l_{h}}}$。同理可得，假设序列针对前提序列的上下文向量$\tilde{p} \in \mathbb{R^{l_{p}}}$。上述过程可由如下公式表示：
$$\tilde{p}_{i} = \sum^{l_{h}}_{j=1}\frac{exp(e_{ij})}{\sum^{l_{p}}_{k=1}exp(e_{ik})}\bar{h}_{j}, \forall i \in [1, ..., l_{p}]$$
$$\tilde{h}_{j} = \sum^{l_{p}}_{i=1}\frac{exp(e_{ij})}{\sum^{l_{h}}_{k=1}exp(e_{kj})}\bar{p}_{i}, \forall j \in [1, ..., l_{h}]$$
&emsp;&emsp;其中$e_{ij}$表示前提序列的第i个元素与假设序列的第j个元素相关程度，计算元素之间关联程度的方式有很多，本文通过最简单的方式，计算两序列中各个词向量之间的内积来衡量两词之间关联程度，公式如下：
$$e_{ij} = \bar{p}_{i}^{T}\bar{b}_{j}$$
&emsp;&emsp;实验表明[@Chen2016]，其他一些更加复杂的衡量词之间关联程度的方式并没有改善实验性能。对于两个序列的各个元素之间的相似度计算可利用张量(Tensor)运算进行并行计算。<br>
&emsp;&emsp;然后模型对序列的各方面的信息进行融合。针对前提序列，本模型对编码后的矩阵$\bar{p} \in \mathbb{R^{l_{p} \times h}}$ 和假设序列的上下文矩阵$\tilde{p} \in \mathbb{R^{l_{p} \times h}}$ 做了一些操作后，之后进行合并，过程如下公式所示：
$$m_{p} = [\bar{p};\tilde{p};\bar{p}-\tilde{p};\bar{p} \odot \tilde{p}]$$
&emsp;&emsp;其中$\odot$表示矩阵中对应位置的元素相乘。同理可得，假设序列的信息融合过程如下公式所示：
$$m_{h} = [\bar{h};\tilde{h};\bar{h}-\tilde{h};\bar{h} \odot \tilde{h}]$$
#### 组合推理模块(Inference Composition)
&emsp;&emsp;局部推理信息形成之后，模型需要对局部推理信息进行进一步的组合，使得每个时刻的元素都包含上下文的信息。本模型
$$v_{p,i} = BiLSTM(m_{p},i), \forall i \in [1, 2, ..., l_{p}]$$ 
$$v_{h,j} = BiLSTM(m_{h},j), \forall j \in [1, 2, ..., l_{h}]$$ 
&emsp;&emsp;最后的推理模块利用池化与合并操作将处理后的两条序列信息转化为一个固定长度的向量，并且通过分类器对两序列的蕴含关系进行分类。池化操作主要是将平均池化和最大池化相结合，然后将池化后的向量进行合并操作，形成一个长度固定的向量$v \in \mathbb{R^{4h}}$。向量$v$的计算过程如下：
$$v_{p,ave} =  \frac{1}{l_{p}}\sum_{i=1}^{l_{p}}v_{p,i}$$
$$v_{p,max} = max_{i=1}^{l_{p}}v_{p,i}$$ 
$$v_{h,ave} =  \frac{1}{l_{h}}\sum_{j=1}^{l_{h}}v_{h,i}$$ 
$$v_{h,max} = max_{i=1}^{l_{h}}v_{h,j}$$ 
$$v = [v_{p,ave};v_{p,max};v_{h,ave};v_{h,max}] \in \mathbb{R^{4h}}$$
&emsp;&emsp;最后，模型分类器利用多层感知机(Multi-Layer Perceptron)对句子蕴含关系进行分类。本模型的多层感知机只有一个隐藏层，激活函数是tanh，最后输出层利用softmax进行数据归一化。分类的计算过程如下：
$$out = softmax(tanh(Wv+b))$$
&emsp;&emsp;其中$W \in \mathbb{R^{3 \times 4h}}$和$b \in \mathbb{R^{3}}$分别是全连接层的权重和偏置，输出向量$out \in \mathbb{R^{3}}, out_i \in (0,1), \forall i \in [1,2,3]$中的每一个元素表示相对应蕴含关系的概率。

### 实验细节与部分实验结果
&emsp;&emsp;本文我们选用 300-D Glove 840B 中预训练的词向量来初始词向量。对于Glove中未出现的词，我们对其词向量进行随机初始化。训练过程中，我们选用Adam来优化梯度下降算法，其中Adam优化器中的超参数$\beta_{1}=0.9, \beta_{2}=0.999$，初始学习率$\eta=0.0004$，每次训练数据的数目$batch\_size=32$。模型参数方面，所有的长短记忆神经网络(LSTM)的隐状态向量和初始词向量的维度都是300维。我们在每层全连接层的输入处应用dropout机制，以加快训练速度并防止过拟合，dropout的比例均为0.5。
&emsp;&emsp;目前，我们分别在数据集SNLI和MultiNLI数据集上进行了训练并测试。训练及测试结果如表所示：
<center>
表：实验结果</br>
<table border="1">
<tr>
<td>Train Corpus</td>
<td>Model</td>
<td>Embedding-Dimention</td>
<td>Params</td>
<td>Train Acc</td>
<td>SNLI Dev Acc</td>
<td>MNLI Dev-matched Acc</td>
<td>MNLI Dev-mismatched Acc</td>
</tr>
</table>
</center>
</br>

### 改进计划
#### 文本表示
&emsp;&emsp;

#### 序列编码

#### 序列元素影响可视化

# 参考文献



